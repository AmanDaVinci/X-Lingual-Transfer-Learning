{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "trainer.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "427nYWFDu75f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "b93d6e4d-18d3-4119-ac72-74ce2bf9a0a6"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May 23 09:32:34 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQxsDT-zuQCu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e317583-f739-489b-f1b7-fc08b5a93d73"
      },
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB is True:\n",
        "    SSH_PRIVATE_KEY = \"\"\"-----BEGIN OPENSSH PRIVATE KEY-----\n",
        "b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW\n",
        "QyNTUxOQAAACBCQSZ90USE74MtLi9mFvmU2vcFDTLBpILy8uZ9xja4lgAAAJAkcwBUJHMA\n",
        "VAAAAAtzc2gtZWQyNTUxOQAAACBCQSZ90USE74MtLi9mFvmU2vcFDTLBpILy8uZ9xja4lg\n",
        "AAAEBpHtV9l0P5/K8Kp/BEHiZu7JnYOukkvsQPi9bWsvGbsUJBJn3RRITvgy0uL2YW+ZTa\n",
        "9wUNMsGkgvLy5n3GNriWAAAACnVzZXJAY2V0dXMBAgM=\n",
        "-----END OPENSSH PRIVATE KEY-----\n",
        "    \"\"\"\n",
        "    REPO_NAME = \"cross-lingual-transfer-learning\"\n",
        "    GIT_PATH = f\"git@github.com:AmanDaVinci/{REPO_NAME}.git\"\n",
        "    BRANCH = \"master\"\n",
        "\n",
        "    %cd /content\n",
        "    !eval `ssh-agent -s` && \\\n",
        "        ssh-add - <<< \"{SSH_PRIVATE_KEY}\" && \\\n",
        "        mkdir -p ~/.ssh/ && \\\n",
        "        ssh-keyscan -H github.com >> ~/.ssh/known_hosts && \\\n",
        "        rm -rf \"{REPO_NAME}\" && \\\n",
        "        git clone -b {BRANCH} --single-branch --depth=1 {GIT_PATH}\n",
        "    \n",
        "    %cd \"/content/{REPO_NAME}/\"\n",
        "    !pip3 install -r requirements.txt\n",
        "else:\n",
        "    import os\n",
        "    if os.path.basename(os.getcwd()) == 'notebooks':\n",
        "        os.chdir(os.path.dirname(os.getcwd()))\n",
        "\n",
        "from configs.environment import init_environment\n",
        "init_environment()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Agent pid 147\n",
            "Identity added: (stdin) (user@cetus)\n",
            "# github.com:22 SSH-2.0-babeld-f24cc9ea\n",
            "# github.com:22 SSH-2.0-babeld-f24cc9ea\n",
            "# github.com:22 SSH-2.0-babeld-f24cc9ea\n",
            "Cloning into 'cross-lingual-transfer-learning'...\n",
            "Warning: Permanently added the RSA host key for IP address '140.82.118.4' to the list of known hosts.\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 39 (delta 6), reused 17 (delta 1), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (39/39), 1.06 MiB | 2.42 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n",
            "/content/cross-lingual-transfer-learning\n",
            "Collecting dirsync==2.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/fa/be/d5d380d68d72fc79dcd47aaf365cb6b50ae05a4cdedfd2f16146e28e13e6/dirsync-2.2.4.tar.gz\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.0.0)\n",
            "Collecting transformers==2.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard==2.2.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (2.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from dirsync==2.2.4->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 2)) (5.6.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 2)) (4.10.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 2)) (7.5.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 2)) (4.7.4)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 2)) (5.2.2)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r requirements.txt (line 2)) (5.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0->-r requirements.txt (line 3)) (1.18.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0->-r requirements.txt (line 3)) (3.0.12)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 55.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0->-r requirements.txt (line 3)) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0->-r requirements.txt (line 3)) (0.7)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 58.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 55.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0->-r requirements.txt (line 3)) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.1->-r requirements.txt (line 4)) (0.9.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.1->-r requirements.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.1->-r requirements.txt (line 4)) (3.10.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.1->-r requirements.txt (line 4)) (1.7.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.1->-r requirements.txt (line 4)) (46.3.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.1->-r requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.1->-r requirements.txt (line 4)) (0.34.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.1->-r requirements.txt (line 4)) (1.29.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.1->-r requirements.txt (line 4)) (1.6.0.post3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.1->-r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (2.1.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (0.4.4)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (4.6.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (0.6.0)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (2.11.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (0.3)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (4.3.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (3.1.5)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (5.0.6)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (4.5.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (5.3.4)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter==1.0.0->-r requirements.txt (line 2)) (3.5.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 2)) (19.0.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter==1.0.0->-r requirements.txt (line 2)) (1.9.0)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (0.8.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter==1.0.0->-r requirements.txt (line 2)) (1.0.18)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0->-r requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0->-r requirements.txt (line 3)) (0.15.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0->-r requirements.txt (line 3)) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0->-r requirements.txt (line 3)) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.1->-r requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.1->-r requirements.txt (line 4)) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.1->-r requirements.txt (line 4)) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.1->-r requirements.txt (line 4)) (3.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard==2.2.1->-r requirements.txt (line 4)) (1.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (20.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (0.5.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (2.6.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (0.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->jupyter==1.0.0->-r requirements.txt (line 2)) (2.8.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter==1.0.0->-r requirements.txt (line 2)) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter==1.0.0->-r requirements.txt (line 2)) (0.1.9)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.1->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.2.1->-r requirements.txt (line 4)) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard==2.2.1->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->bleach->nbconvert->jupyter==1.0.0->-r requirements.txt (line 2)) (2.4.7)\n",
            "Building wheels for collected packages: dirsync, sacremoses\n",
            "  Building wheel for dirsync (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dirsync: filename=dirsync-2.2.4-cp36-none-any.whl size=11902 sha256=03713368d501776fa5c7759009e7d1a08a18a31331fe016308c9e3002948502a\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/f6/37/736500e07ab9ea2b7f9fd9feff6032cf45deff9b5a6bc6c469\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=c77c4196cc8ad5f7ea87979d1fcda23aff7161844418bfb095a0a64d341088da\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built dirsync sacremoses\n",
            "Installing collected packages: dirsync, tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed dirsync-2.2.4 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.9.0\n",
            "IN_COLAB: True\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrDaNbtz4KKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /content/gdrive/My\\ Drive/NLP2\\ Xlingual/best_checkpoints/ ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJF0zuoY4jcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/cross-lingual-transfer-learning/best_checkpoints /content/cross-lingual-transfer-learning/checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "rlR9gUiUuQC8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "outputId": "f4616302-60a1-4586-bb27-6cd3b9ab6dfe"
      },
      "source": [
        "!python3 /content/cross-lingual-transfer-learning/main.py --config=configs.finetune_bert --test"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-23 09:56:37.699465: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "INFO:root:Config :\n",
            "{'exp_name': 'finetune_bert', 'bert_arch': 'bert-base-multilingual-cased', 'epochs': 3, 'batch_size': 8, 'lr': 5e-05, 'valid_freq': 50, 'save_freq': 100, 'device': 'cuda', 'data_dir': 'data/', 'max_grad_norm': 1.0, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'freeze_gradually': False, 'freeze_static_layers': 0}\n",
            "INFO:root:Using device: cuda\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
            "INFO:transformers.configuration_utils:Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_attentions\": true,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at cache/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "INFO:transformers.modeling_utils:Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
            "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at cache/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "INFO:root:Loading checkpoint from checkpoints/finetune_bert/best-model.pt\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "INFO:root:[Test]\tPerplexity: 7.342 Total Loss: 1.994\n",
            "[Test]\tPerplexity: 7.342 Total Loss: 1.994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AZKNBY7wcHd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "outputId": "2dedc2a8-8afb-4fb9-a54f-fd6911bf0830"
      },
      "source": [
        "!python3 /content/cross-lingual-transfer-learning/main.py --config=configs.finetune_bert_06 --test"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-23 10:03:18.192590: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "INFO:root:Config :\n",
            "{'exp_name': 'finetune_bert_06', 'bert_arch': 'bert-base-multilingual-cased', 'epochs': 3, 'batch_size': 8, 'lr': 5e-05, 'valid_freq': 50, 'save_freq': 100, 'device': 'cuda', 'data_dir': 'data/', 'max_grad_norm': 1.0, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'freeze_gradually': False, 'freeze_static_layers': 6}\n",
            "INFO:root:Using device: cuda\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
            "INFO:transformers.configuration_utils:Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_attentions\": true,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at cache/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "INFO:transformers.modeling_utils:Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
            "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at cache/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "INFO:root:Loading checkpoint from checkpoints/finetune_bert_06/best-model.pt\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "INFO:root:[Test]\tPerplexity: 9.045 Total Loss: 2.202\n",
            "[Test]\tPerplexity: 9.045 Total Loss: 2.202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFY7jm52_JmD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "outputId": "64adbc60-48de-494a-c9d9-0b7b03f30ca1"
      },
      "source": [
        "!python3 /content/cross-lingual-transfer-learning/main.py --config=configs.finetune_bert_09 --test"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-23 10:06:05.229013: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "INFO:root:Config :\n",
            "{'exp_name': 'finetune_bert_09', 'bert_arch': 'bert-base-multilingual-cased', 'epochs': 3, 'batch_size': 8, 'lr': 5e-05, 'valid_freq': 100, 'save_freq': 1000, 'device': 'cuda', 'data_dir': 'data/', 'max_grad_norm': 1.0, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'freeze_per_epoch': False, 'freeze_static_layers': 9}\n",
            "INFO:root:Using device: cuda\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
            "INFO:transformers.configuration_utils:Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_attentions\": true,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at cache/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "INFO:transformers.modeling_utils:Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
            "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at cache/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "INFO:root:Loading checkpoint from checkpoints/finetune_bert_09/best-model.pt\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "INFO:root:[Test]\tPerplexity: 14.159 Total Loss: 2.650\n",
            "[Test]\tPerplexity: 14.159 Total Loss: 2.650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXT62PtX_yU4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "outputId": "650bccf2-a13d-4dbf-c6e2-b3abef5cf576"
      },
      "source": [
        "!python3 /content/cross-lingual-transfer-learning/main.py --config=configs.finetune_bert_11 --test"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-23 10:08:57.297389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "INFO:root:Config :\n",
            "{'exp_name': 'finetune_bert_11', 'bert_arch': 'bert-base-multilingual-cased', 'epochs': 3, 'batch_size': 8, 'lr': 5e-05, 'valid_freq': 50, 'save_freq': 100, 'device': 'cuda', 'data_dir': 'data/', 'max_grad_norm': 1.0, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'freeze_gradually': False, 'freeze_static_layers': 11}\n",
            "INFO:root:Using device: cuda\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
            "INFO:transformers.configuration_utils:Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_attentions\": true,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at cache/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "INFO:transformers.modeling_utils:Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
            "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at cache/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "INFO:root:Loading checkpoint from checkpoints/finetune_bert_11/best-model.pt\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "INFO:root:[Test]\tPerplexity: 16.110 Total Loss: 2.779\n",
            "[Test]\tPerplexity: 16.110 Total Loss: 2.779\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-t1foZ4Acbp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "outputId": "fd877c8f-cd93-498f-9ff7-4e8239692538"
      },
      "source": [
        "!python3 /content/cross-lingual-transfer-learning/main.py --config=configs.finetune_bert_gradual --test"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-23 10:11:26.846844: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "INFO:root:Config :\n",
            "{'exp_name': 'finetune_bert_gradual', 'bert_arch': 'bert-base-multilingual-cased', 'epochs': 3, 'batch_size': 8, 'lr': 5e-05, 'valid_freq': 50, 'save_freq': 100, 'device': 'cuda', 'data_dir': 'data/', 'max_grad_norm': 1.0, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'freeze_gradually': True, 'freeze_static_layers': 0}\n",
            "INFO:root:Using device: cuda\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
            "INFO:transformers.configuration_utils:Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_attentions\": true,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at cache/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "INFO:transformers.modeling_utils:Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
            "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at cache/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "INFO:root:Loading checkpoint from checkpoints/finetune_bert_gradual/best-model.pt\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "INFO:root:[Test]\tPerplexity: 8.950 Total Loss: 2.192\n",
            "[Test]\tPerplexity: 8.950 Total Loss: 2.192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahW4mDnxBA3p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "outputId": "9cddd843-d056-4be7-c126-9e47217ede86"
      },
      "source": [
        "!python3 /content/cross-lingual-transfer-learning/main.py --config=configs.bertje --test"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-23 10:17:54.905116: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "INFO:root:Config :\n",
            "{'exp_name': 'bertje', 'bert_arch': 'bert-base-dutch-cased', 'epochs': 3, 'batch_size': 8, 'lr': 5e-05, 'valid_freq': 50, 'save_freq': 100, 'device': 'cuda', 'data_dir': 'data/', 'max_grad_norm': 1.0, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'freeze_gradually': False, 'freeze_static_layers': 12}\n",
            "INFO:root:Using device: cuda\n",
            "INFO:filelock:Lock 139983717680352 acquired on /root/.cache/torch/transformers/6702c5c53edb76b65d71f73ff2d9811ba62f16257ea58e36dedceffd71290a6a.1a78bd120fe46d78b55efa59f4ffa1dafcc9242743ab9fd6629d1b56672c9119.lock\n",
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp5p_jl25a\n",
            "Downloading: 100% 433/433 [00:00<00:00, 358kB/s]\n",
            "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/config.json in cache at /root/.cache/torch/transformers/6702c5c53edb76b65d71f73ff2d9811ba62f16257ea58e36dedceffd71290a6a.1a78bd120fe46d78b55efa59f4ffa1dafcc9242743ab9fd6629d1b56672c9119\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/6702c5c53edb76b65d71f73ff2d9811ba62f16257ea58e36dedceffd71290a6a.1a78bd120fe46d78b55efa59f4ffa1dafcc9242743ab9fd6629d1b56672c9119\n",
            "INFO:filelock:Lock 139983717680352 released on /root/.cache/torch/transformers/6702c5c53edb76b65d71f73ff2d9811ba62f16257ea58e36dedceffd71290a6a.1a78bd120fe46d78b55efa59f4ffa1dafcc9242743ab9fd6629d1b56672c9119.lock\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/config.json from cache at /root/.cache/torch/transformers/6702c5c53edb76b65d71f73ff2d9811ba62f16257ea58e36dedceffd71290a6a.1a78bd120fe46d78b55efa59f4ffa1dafcc9242743ab9fd6629d1b56672c9119\n",
            "INFO:transformers.configuration_utils:Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_attentions\": true,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n",
            "INFO:filelock:Lock 139983717410128 acquired on cache/e5754f612ca0f16edba5b775fdddba806751f5e4b87c5e7f16cc0c8d8d17df4d.b7c03627733fd0712f078a4d3a31ad964550f50a6113efdf874ecbcf5ddf6b53.lock\n",
            "INFO:transformers.file_utils:https://cdn.huggingface.co/wietsedv/bert-base-dutch-cased/pytorch_model.bin not found in cache or force_download set to True, downloading to /content/cross-lingual-transfer-learning/cache/tmp13f67in_\n",
            "Downloading: 100% 439M/439M [00:05<00:00, 83.4MB/s]\n",
            "INFO:transformers.file_utils:storing https://cdn.huggingface.co/wietsedv/bert-base-dutch-cased/pytorch_model.bin in cache at cache/e5754f612ca0f16edba5b775fdddba806751f5e4b87c5e7f16cc0c8d8d17df4d.b7c03627733fd0712f078a4d3a31ad964550f50a6113efdf874ecbcf5ddf6b53\n",
            "INFO:transformers.file_utils:creating metadata file for cache/e5754f612ca0f16edba5b775fdddba806751f5e4b87c5e7f16cc0c8d8d17df4d.b7c03627733fd0712f078a4d3a31ad964550f50a6113efdf874ecbcf5ddf6b53\n",
            "INFO:filelock:Lock 139983717410128 released on cache/e5754f612ca0f16edba5b775fdddba806751f5e4b87c5e7f16cc0c8d8d17df4d.b7c03627733fd0712f078a4d3a31ad964550f50a6113efdf874ecbcf5ddf6b53.lock\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/wietsedv/bert-base-dutch-cased/pytorch_model.bin from cache at cache/e5754f612ca0f16edba5b775fdddba806751f5e4b87c5e7f16cc0c8d8d17df4d.b7c03627733fd0712f078a4d3a31ad964550f50a6113efdf874ecbcf5ddf6b53\n",
            "INFO:transformers.modeling_utils:Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
            "INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "INFO:filelock:Lock 139983563493272 acquired on cache/75d9be4cc7910048b3bdd477c435ffc46330193705f74eaf9a4f375cd3be28b2.1e00a56207196ed1759c49bdd1fa93c2fb20273d59fabb0c4c8092f7beb773c2.lock\n",
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/vocab.txt not found in cache or force_download set to True, downloading to /content/cross-lingual-transfer-learning/cache/tmpa5vy2lff\n",
            "Downloading: 100% 241k/241k [00:00<00:00, 719kB/s]\n",
            "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/vocab.txt in cache at cache/75d9be4cc7910048b3bdd477c435ffc46330193705f74eaf9a4f375cd3be28b2.1e00a56207196ed1759c49bdd1fa93c2fb20273d59fabb0c4c8092f7beb773c2\n",
            "INFO:transformers.file_utils:creating metadata file for cache/75d9be4cc7910048b3bdd477c435ffc46330193705f74eaf9a4f375cd3be28b2.1e00a56207196ed1759c49bdd1fa93c2fb20273d59fabb0c4c8092f7beb773c2\n",
            "INFO:filelock:Lock 139983563493272 released on cache/75d9be4cc7910048b3bdd477c435ffc46330193705f74eaf9a4f375cd3be28b2.1e00a56207196ed1759c49bdd1fa93c2fb20273d59fabb0c4c8092f7beb773c2.lock\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/wietsedv/bert-base-dutch-cased/vocab.txt from cache at cache/75d9be4cc7910048b3bdd477c435ffc46330193705f74eaf9a4f375cd3be28b2.1e00a56207196ed1759c49bdd1fa93c2fb20273d59fabb0c4c8092f7beb773c2\n",
            ">>>BERTje\n",
            "INFO:root:[Test]\tPerplexity: 23.547 Total Loss: 3.159\n",
            "[Test]\tPerplexity: 23.547 Total Loss: 3.159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8Ln6AXjCfFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}