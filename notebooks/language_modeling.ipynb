{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language_modeling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0fuXFI6OBcQ",
        "colab_type": "text"
      },
      "source": [
        "# Language Modelling Experiments\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5lgbhChRHaG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "c600ca59-5ee3-46b3-fc6e-88197d630650"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 26285 (delta 32), reused 41 (delta 12), pack-reused 26213\u001b[K\n",
            "Receiving objects: 100% (26285/26285), 15.40 MiB | 14.62 MiB/s, done.\n",
            "Resolving deltas: 100% (18347/18347), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJMptNm7RabX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "afbb77b8-2f8d-4d08-9d4e-5e74691a5943"
      },
      "source": [
        "!pip install transformers/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./transformers\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.18.3)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 41.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.4.5.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (0.14.1)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.8.0-cp36-none-any.whl size=596121 sha256=9479b51622f73e691127d60c2f00c21710846e87d2ce8e5ab145fb9289fac3ca\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dnu6wkm6/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=c053f462664ea10249ea2fcaa911184b2321d62ebb8338813684bd3c95ad2435\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.7.0 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcjWs3xKRmSN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "baaa2eb9-773e-4213-8aa1-b7c85f79b4fa"
      },
      "source": [
        "!pip install -r transformers/examples/requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
            "\r\u001b[K     |█▊                              | 10kB 17.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 20kB 4.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 40kB 6.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 51kB 5.3MB/s eta 0:00:01\r\u001b[K     |██████████                      | 61kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 71kB 5.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 81kB 6.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 92kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 102kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 112kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 122kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 133kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 143kB 7.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 153kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 163kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 174kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 184kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 194kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r transformers/examples/requirements.txt (line 2)) (2.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r transformers/examples/requirements.txt (line 3)) (0.22.2.post1)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from -r transformers/examples/requirements.txt (line 5)) (5.4.8)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/9d/9846507837ca50ae20917f59d83b79246b8313bd19d4f5bf575ecb98132b/sacrebleu-1.4.9-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/6d/2b9a64cba1e4e6ecd4effbf6834b2592b54dc813654f84029758e5daeeb5/rouge_score-0.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (from -r transformers/examples/requirements.txt (line 8)) (2.1.0)\n",
            "Collecting pytorch-lightning==0.7.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/53/0549dd9c44c90e96d217592e094e9c53ef39ae2fed0c5cdb7e57aca65af6/pytorch_lightning-0.7.3-py3-none-any.whl (203kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 18.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r transformers/examples/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r transformers/examples/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX->-r transformers/examples/requirements.txt (line 1)) (1.18.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (1.7.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (1.6.0.post3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (0.34.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (1.28.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r transformers/examples/requirements.txt (line 2)) (46.1.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r transformers/examples/requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r transformers/examples/requirements.txt (line 3)) (0.14.1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->-r transformers/examples/requirements.txt (line 4)) (2.3.1)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu->-r transformers/examples/requirements.txt (line 6)) (3.6.6)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge-score->-r transformers/examples/requirements.txt (line 7)) (3.2.5)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r transformers/examples/requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r transformers/examples/requirements.txt (line 8)) (4.38.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r transformers/examples/requirements.txt (line 8)) (19.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r transformers/examples/requirements.txt (line 8)) (0.3.1.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r transformers/examples/requirements.txt (line 8)) (0.21.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r transformers/examples/requirements.txt (line 8)) (1.12.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r transformers/examples/requirements.txt (line 8)) (2.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r transformers/examples/requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.3->-r transformers/examples/requirements.txt (line 9)) (1.5.0+cu101)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r transformers/examples/requirements.txt (line 2)) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r transformers/examples/requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r transformers/examples/requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r transformers/examples/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r transformers/examples/requirements.txt (line 2)) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r transformers/examples/requirements.txt (line 2)) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r transformers/examples/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r transformers/examples/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r transformers/examples/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r transformers/examples/requirements.txt (line 4)) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r transformers/examples/requirements.txt (line 4)) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r transformers/examples/requirements.txt (line 4)) (3.13)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets->-r transformers/examples/requirements.txt (line 8)) (1.51.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard->-r transformers/examples/requirements.txt (line 2)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r transformers/examples/requirements.txt (line 2)) (3.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=ac27a4f14c7e9e5a32b790933ea063ce3e14f55fd6cf0243c28312a8a8fb12a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "\u001b[31mERROR: pytorch-lightning 0.7.3 has requirement future>=0.17.1, but you'll have future 0.16.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pytorch-lightning 0.7.3 has requirement tqdm>=4.41.0, but you'll have tqdm 4.38.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboardX, seqeval, portalocker, sacrebleu, rouge-score, pytorch-lightning\n",
            "Successfully installed portalocker-1.7.0 pytorch-lightning-0.7.3 rouge-score-0.0.3 sacrebleu-1.4.9 seqeval-0.0.12 tensorboardX-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPXrVxgASkKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir /content/output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJzSL6nwOveB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30c72a03-eab8-4f45-c7e2-22f4a4df7a7a"
      },
      "source": [
        "!python3 /content/transformers/examples/run_language_modeling.py \\\n",
        "    --output_dir=/content/output \\\n",
        "    --model_type=bert \\\n",
        "    --model_name_or_path=bert-base-multilingual-cased \\\n",
        "    --do_train \\\n",
        "    --train_data_file=/content/train.txt \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=/content/valid.txt \\\n",
        "    --mlm"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-05-05 06:02:23.638618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "05/05/2020 06:02:25 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
            "05/05/2020 06:02:25 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "05/05/2020 06:02:25 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='/content/output', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir=None, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1)\n",
            "05/05/2020 06:02:25 - INFO - filelock -   Lock 140509788796128 acquired on /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0.lock\n",
            "05/05/2020 06:02:25 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp44igrrd1\n",
            "Downloading: 100% 625/625 [00:00<00:00, 476kB/s]\n",
            "05/05/2020 06:02:26 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json in cache at /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
            "05/05/2020 06:02:26 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
            "05/05/2020 06:02:26 - INFO - filelock -   Lock 140509788796128 released on /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0.lock\n",
            "05/05/2020 06:02:26 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
            "05/05/2020 06:02:26 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "05/05/2020 06:02:26 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json from cache at /root/.cache/torch/transformers/45629519f3117b89d89fd9c740073d8e4c1f0a70f9842476185100a8afe715d1.65df3cef028a0c91a7b059e4c404a975ebe6843c71267b67019c0e9cfa8a88f0\n",
            "05/05/2020 06:02:26 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "05/05/2020 06:02:27 - INFO - filelock -   Lock 140509414812864 acquired on /root/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729.lock\n",
            "05/05/2020 06:02:27 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp32ind5hf\n",
            "Downloading: 100% 996k/996k [00:00<00:00, 2.03MB/s]\n",
            "05/05/2020 06:02:27 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt in cache at /root/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "05/05/2020 06:02:27 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "05/05/2020 06:02:27 - INFO - filelock -   Lock 140509414812864 released on /root/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729.lock\n",
            "05/05/2020 06:02:27 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt from cache at /root/.cache/torch/transformers/96435fa287fbf7e469185f1062386e05a075cadbf6838b74da22bf64b080bc32.99bcd55fc66f4f3360bc49ba472b940b8dcf223ea6a345deb969d607ca900729\n",
            "05/05/2020 06:02:28 - INFO - filelock -   Lock 140509414883736 acquired on /root/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059.lock\n",
            "05/05/2020 06:02:28 - INFO - transformers.file_utils -   https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp85cx058y\n",
            "Downloading: 100% 714M/714M [00:15<00:00, 46.0MB/s]\n",
            "05/05/2020 06:02:44 - INFO - transformers.file_utils -   storing https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin in cache at /root/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "05/05/2020 06:02:44 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "05/05/2020 06:02:44 - INFO - filelock -   Lock 140509414883736 released on /root/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059.lock\n",
            "05/05/2020 06:02:44 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-multilingual-cased-pytorch_model.bin from cache at /root/.cache/torch/transformers/3d1d2b2daef1e2b3ddc2180ddaae8b7a37d5f279babce0068361f71cd548f615.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059\n",
            "05/05/2020 06:02:53 - INFO - transformers.modeling_utils -   Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
            "05/05/2020 06:02:53 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "05/05/2020 06:02:53 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at /content\n",
            "05/05/2020 06:03:22 - INFO - transformers.data.datasets.language_modeling -   Saving features into cached file /content/cached_lm_BertTokenizer_510_train.txt [took 0.078 s]\n",
            "05/05/2020 06:03:22 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at /content\n",
            "05/05/2020 06:03:26 - INFO - transformers.data.datasets.language_modeling -   Saving features into cached file /content/cached_lm_BertTokenizer_510_valid.txt [took 0.009 s]\n",
            "05/05/2020 06:03:26 - INFO - transformers.trainer -   You are instantiating a Trainer but wandb is not installed. Install it to use Weights & Biases logging.\n",
            "05/05/2020 06:03:36 - INFO - transformers.trainer -   ***** Running training *****\n",
            "05/05/2020 06:03:36 - INFO - transformers.trainer -     Num examples = 4757\n",
            "05/05/2020 06:03:36 - INFO - transformers.trainer -     Num Epochs = 3\n",
            "05/05/2020 06:03:36 - INFO - transformers.trainer -     Instantaneous batch size per GPU = 8\n",
            "05/05/2020 06:03:36 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "05/05/2020 06:03:36 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "05/05/2020 06:03:36 - INFO - transformers.trainer -     Total optimization steps = 1785\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/595 [00:00<?, ?it/s]\u001b[A/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha)\n",
            "\n",
            "Iteration:   0% 1/595 [00:01<13:08,  1.33s/it]\u001b[ATraceback (most recent call last):\n",
            "  File \"/content/transformers/examples/run_language_modeling.py\", line 284, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/run_language_modeling.py\", line 254, in main\n",
            "    trainer.train(model_path=model_path)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\", line 342, in train\n",
            "    tr_loss += self._training_step(model, inputs, optimizer)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/trainer.py\", line 431, in _training_step\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 198, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 100, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 1.82 GiB (GPU 0; 15.90 GiB total capacity; 12.40 GiB already allocated; 1.08 GiB free; 14.04 GiB reserved in total by PyTorch) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:289)\n",
            "frame #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7fcb40b9b536 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so)\n",
            "frame #1: <unknown function> + 0x1cf1e (0x7fcb40de4f1e in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10_cuda.so)\n",
            "frame #2: <unknown function> + 0x1df9e (0x7fcb40de5f9e in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10_cuda.so)\n",
            "frame #3: THCStorage_resize + 0x96 (0x7fcb42084c26 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\n",
            "frame #4: THCTensor_resizeNd + 0x441 (0x7fcb42095de1 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\n",
            "frame #5: THNN_CudaClassNLLCriterion_updateGradInput + 0x6a (0x7fcb4293986a in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\n",
            "frame #6: <unknown function> + 0x1039328 (0x7fcb42030328 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\n",
            "frame #7: <unknown function> + 0xfbd4cb (0x7fcb41fb44cb in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\n",
            "frame #8: <unknown function> + 0x10c4b13 (0x7fcb7996db13 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #9: <unknown function> + 0x2b49658 (0x7fcb7b3f2658 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #10: <unknown function> + 0x10c4b13 (0x7fcb7996db13 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #11: torch::autograd::generated::NllLossBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x276 (0x7fcb7b14ddb6 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #12: <unknown function> + 0x2d89c05 (0x7fcb7b632c05 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #13: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7fcb7b62ff03 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #14: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7fcb7b630ce2 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #15: torch::autograd::Engine::thread_init(int) + 0x39 (0x7fcb7b629359 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #16: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7fcb87d68378 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #17: <unknown function> + 0xbd6df (0x7fcb8d6296df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\n",
            "frame #18: <unknown function> + 0x76db (0x7fcb8e70b6db in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
            "frame #19: clone + 0x3f (0x7fcb8ea4488f in /lib/x86_64-linux-gnu/libc.so.6)\n",
            "\n",
            "\n",
            "Epoch:   0% 0/3 [00:01<?, ?it/s]\n",
            "Iteration:   0% 1/595 [00:01<15:38,  1.58s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUp_fwzWXbV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2hYS466hokF",
        "colab_type": "text"
      },
      "source": [
        "We have chosen to experiment with the few-shot finetuning technique first.\n",
        "\n",
        "We form the research question as: \n",
        "Given a model architecture, how does the following compare:\n",
        "1. model trained on the target language from scratch (native)\n",
        "2. model trained on a multilingual dataset and then finetuned on target (transferred)\n",
        "\n",
        "Our native model might be a trained RobBERT - a RoBERTa-based model available online - trained exclusively on the OSCAR dutch dataset.\n",
        "Our transferred model might be a XLM-RoBERTa - RoBERTa-based model trained on multilingual data (CommonCrawl 100 languages).\n",
        "\n",
        "Using the transformers library, we will finetune the pre-trained XLM-RoBERTa on the same OSCAR dutch dataset as the native model for fair comparison.\n",
        "\n",
        "One major pain point is that the OSCAR dutch data is quite large (39 GB). Do you suggest going this route?\n",
        "\n",
        "Or should we stick to the following configuration?\n",
        "1. Native Model: BERTje\n",
        "2. Transferred Model: BERT-base Multilingual\n",
        "3. Finetuning dataset provided by you\n",
        "\n",
        "Since the above configuration might take less time to train, we will be able to focus more on doing better comparative experiments."
      ]
    }
  ]
}